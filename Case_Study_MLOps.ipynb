{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABHIRAM199/Gen-AI/blob/main/Case_Study_MLOps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Case Study : Housing Price Prediction**"
      ],
      "metadata": {
        "id": "jy6JmPRc0iXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agenda**\n",
        "- Data Preprocessing\n",
        "- Model Training\n",
        "- Version Control (Code, Data, and Model)\n",
        "- Model Evaluation\n",
        "- Model Deployment\n",
        "- Monitoring and Logging\n",
        "\n"
      ],
      "metadata": {
        "id": "nVD80eThyMUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLOps pipeline for Housing Price Prediction involves several critical steps, including data preprocessing, model training, versioning, and deployment. Below is a detailed guide that walks through the entire pipeline step-by-step with code snippets for each part.\n",
        "\n",
        "**Case Study Overview:**\n",
        "- **Problem:** Predict housing prices based on various features (like the number of rooms, location, etc.).\n",
        "- **Solution:** Build an MLOps pipeline that automates data preprocessing, model training, versioning (with Git, DVC, and MLflow), and deployment (using Docker, Flask, and Airflow for orchestration)."
      ],
      "metadata": {
        "id": "LldbXQAcyNr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data Preprocessing**\n"
      ],
      "metadata": {
        "id": "ChEdtXOXyl5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step focuses on preparing raw data for model training by handling missing values, encoding categorical features, and feature scaling."
      ],
      "metadata": {
        "id": "mbb4Xe8r7ODy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python Code:**"
      ],
      "metadata": {
        "id": "XpWCYKvEysii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/preprocess.py\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def setup_logging(log_dir='logs', log_file='preprocess.log'):\n",
        "    \"\"\"\n",
        "    Sets up logging configuration.\n",
        "    Ensures that the log directory exists.\n",
        "    \"\"\"\n",
        "    os.makedirs(log_dir, exist_ok=True)  # Create log directory if it doesn't exist\n",
        "    log_path = os.path.join(log_dir, log_file)\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s %(levelname)s:%(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_path),\n",
        "            logging.StreamHandler(sys.stdout)  # Also log to stdout for real-time monitoring\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def ensure_directory_exists(file_path):\n",
        "    \"\"\"\n",
        "    Ensures that the directory for the given file path exists.\n",
        "    If it doesn't exist, the directory is created.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): The file path for which to ensure the directory exists.\n",
        "    \"\"\"\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if directory:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        logging.debug(f\"Ensured directory exists: {directory}\")\n",
        "\n",
        "def preprocess_data(input_path, output_path):\n",
        "    \"\"\"\n",
        "    Preprocesses the housing data by handling missing values and encoding categorical variables.\n",
        "\n",
        "    Parameters:\n",
        "    - input_path (str): Path to the raw input CSV file.\n",
        "    - output_path (str): Path to save the preprocessed CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Loading data from %s\", input_path)\n",
        "        data = pd.read_csv('/content/Housing.csv')\n",
        "\n",
        "        logging.info(\"Handling missing values\")\n",
        "        data.ffill(inplace=True)  # Forward fill to handle missing values\n",
        "\n",
        "        logging.info(\"Encoding categorical variables\")\n",
        "        data = pd.get_dummies(data, drop_first=True)  # One-hot encode categorical variables\n",
        "\n",
        "        logging.info(\"Ensuring output directory exists\")\n",
        "        ensure_directory_exists(output_path)\n",
        "\n",
        "        logging.info(\"Saving preprocessed data to %s\", output_path)\n",
        "        data.to_csv(output_path, index=False)  # Save preprocessed data\n",
        "        logging.info(\"Preprocessing completed successfully.\")\n",
        "\n",
        "    except FileNotFoundError as fnf_error:\n",
        "        logging.error(\"File not found: %s\", fnf_error)\n",
        "        sys.exit(1)  # Exit the script with an error code\n",
        "    except pd.errors.EmptyDataError:\n",
        "        logging.error(\"No data: The file is empty.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in preprocessing data: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup logging\n",
        "    setup_logging()\n",
        "\n",
        "    # Define input and output paths (using the correct uploaded dataset path)\n",
        "    input_data_path = '/content/Housing.csv'  # Make sure this is the correct file path\n",
        "    output_data_path = '/mnt/data/preprocessed_housing.csv'  # Output will be saved here\n",
        "\n",
        "    logging.info(\"Starting the preprocessing pipeline.\")\n",
        "    preprocess_data(input_path=input_data_path, output_path=output_data_path)\n",
        "    logging.info(\"Preprocessing pipeline finished.\")\n"
      ],
      "metadata": {
        "id": "5yG9IPTYyMip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- **Data Loading:** Reads the raw housing dataset (data/raw_housing.csv).\n",
        "- **Missing Value Handling:** Fills missing values using forward-fill (ffill).\n",
        "- **Categorical Encoding:** Encodes categorical variables using one-hot encoding.\n",
        "- **Data Export:** Saves the processed dataset (data/preprocessed_housing.csv)."
      ],
      "metadata": {
        "id": "i-rB5VquyNEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Training**\n"
      ],
      "metadata": {
        "id": "mR6utAox_Bhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this phase, the preprocessed data is split into training and testing sets, and a RandomForest model is trained to predict housing prices.\n",
        "\n",
        "- **Python Code:**"
      ],
      "metadata": {
        "id": "qmMZnbNl7S9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT0Fynf-_Lsj",
        "outputId": "99c2b019-946b-442f-b9d6-9cac6a5103c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.16.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==2.16.2 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.16.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<18,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (16.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.5.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.35)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (2.2.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.2->mlflow)\n",
            "  Downloading databricks_sdk-0.33.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.16.2->mlflow)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (8.4.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (1.27.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (24.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (3.20.3)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow) (0.5.1)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow) (2.27.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.16.2->mlflow) (3.20.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow) (1.2.14)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow) (0.48b0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4->mlflow) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.2->mlflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.2->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.2->mlflow) (2024.8.30)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow) (1.16.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.16.2-py3-none-any.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.16.2-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.33.0-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.0/563.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: aniso8601, smmap, Mako, gunicorn, graphql-core, graphql-relay, gitdb, docker, alembic, graphene, gitpython, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 aniso8601-9.0.1 databricks-sdk-0.33.0 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.4 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.16.2 mlflow-skinny-2.16.2 smmap-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eQbIQCgxddt",
        "outputId": "a5cb3cee-8446-4992-c34a-22fe4cefc5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model R^2 Score: 61.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/10/07 08:02:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/Housing.csv\n"
          ]
        }
      ],
      "source": [
        "# src/train.py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import joblib\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "def train_model(data_path, model_path):\n",
        "    # Load data\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "    # Identify feature columns and target\n",
        "    X = data.drop('price', axis=1)  # Features\n",
        "    y = data['price']  # Target variable\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Define preprocessing for numerical data\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Define preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine preprocessing steps\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create a pipeline that first preprocesses the data and then trains the model\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Start MLflow run for logging\n",
        "    with mlflow.start_run():\n",
        "        # Train the model pipeline\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model\n",
        "        accuracy = pipeline.score(X_test, y_test)\n",
        "        print(f\"Model R^2 Score: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        # Log metrics and model to MLflow\n",
        "        mlflow.log_metric(\"r2_score\", accuracy)\n",
        "        mlflow.sklearn.log_model(pipeline, \"model\")\n",
        "\n",
        "        # Save model using joblib\n",
        "        joblib.dump(pipeline, model_path)\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model(data_path='/content/Housing.csv', model_path='/content/Housing.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- **Libraries Import:** Key libraries such as pandas for data manipulation, sklearn for machine learning, mlflow for experiment tracking, and joblib for model saving are imported.\n",
        "\n",
        "- **Data Loading:** The dataset is read from a CSV file using pd.read_csv().\n",
        "\n",
        "- **Feature and Target Separation:** The target variable (price) is separated from the feature variables (X).\n",
        "\n",
        "- **Categorical and Numerical Columns:** Categorical and numerical columns are identified to apply the appropriate preprocessing techniques.\n",
        "\n",
        "- **Data Splitting:** The dataset is split into training and testing sets using train_test_split().\n",
        "\n",
        "\n",
        "**Preprocessing Pipelines:**\n",
        "- **Numerical Data:** Scaled using StandardScaler.\n",
        "- **Categorical Data:** One-hot encoded using OneHotEncoder.\n",
        "- **Combined Preprocessing:** Both numerical and categorical transformers are combined using ColumnTransformer.\n",
        "\n",
        "- **Pipeline:** A Pipeline is built that applies preprocessing followed by training a RandomForestRegressor.\n",
        "\n",
        "- **Model Training:** The pipeline is trained using the training data.\n",
        "\n",
        "- **Model Evaluation:** The model is evaluated on the test data, and the R^2 score is printed.\n",
        "\n",
        "- **MLflow Logging:** Model performance and the model itself are logged with MLflow.\n",
        "\n",
        "- **Model Saving:** The trained model is saved using joblib.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jwEcI9PTAoKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version Control**\n",
        "- Versioning is crucial in MLOps pipelines for tracking code, data, and model versions. Here’s how to manage version control for each component.\n",
        "\n",
        "**a. Code Versioning (Git)**\n",
        "- Initialize a Git repository for versioning code.\n",
        "\n",
        "```\n",
        "# Initialize Git\n",
        "git init\n",
        "git add .\n",
        "git commit -m \"Initial commit: Data Preprocessing and Model Training\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hcidPWgPBQfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Data Versioning (DVC)**\n",
        "- DVC (Data Version Control) is used to track the dataset and the preprocessed data.\n",
        "\n",
        "```\n",
        "# Initialize DVC\n",
        "dvc init\n",
        "\n",
        "# Add raw data to DVC\n",
        "dvc add data/raw_housing.csv\n",
        "git add data/raw_housing.csv.dvc .gitignore\n",
        "git commit -m \"Add raw housing data\"\n",
        "\n",
        "# Add preprocessed data to DVC\n",
        "dvc add data/preprocessed_housing.csv\n",
        "git add data/preprocessed_housing.csv.dvc\n",
        "git commit -m \"Add preprocessed housing data\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "_EPKypO5BcLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Model Versioning (MLflow)**\n",
        "- The trained model is versioned using MLflow, logged during the training process. You can track model versions in MLflow’s UI.\n",
        "\n",
        "```\n",
        "mlflow ui  # Launch the MLflow UI to monitor model versions\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qR2e3pFMBnGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Evaluation**\n",
        "After training the model, evaluation metrics are logged to MLflow for easy access and comparison across versions.\n",
        "\n",
        "- **Metric Logged:** Model accuracy.\n",
        "- **MLflow Tracking:** Stores metrics, parameters, and artifacts for future reference.\n"
      ],
      "metadata": {
        "id": "K68Px9EDBx0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model Deployment**\n"
      ],
      "metadata": {
        "id": "mDUFzz-aV0Px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, it needs to be deployed as a REST API using Flask and Docker for serving predictions.\n",
        "\n",
        "- **Python Code for Flask API:**"
      ],
      "metadata": {
        "id": "oOeLPOLL7Yla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/deploy.py\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "model = joblib.load('/content/Housing.csv')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.json['data']\n",
        "    prediction = model.predict(np.array(data).reshape(1, -1))\n",
        "    return jsonify({'prediction': prediction.tolist()})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxyTVn7uACri",
        "outputId": "0b876ab9-b471-4366-c966-b1e277df03d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- **Flask API:** Exposes an endpoint /predict for model predictions."
      ],
      "metadata": {
        "id": "sKQOJx3waGzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Monitoring and Logging**\n"
      ],
      "metadata": {
        "id": "zfBhP_HmaOjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLflow and Flask logs can be monitored to track model performance, API requests, and errors.\n",
        "\n",
        "- **MLflow:** Logs model metrics and versioning.\n",
        "- **Flask:** Logs API requests and model predictions."
      ],
      "metadata": {
        "id": "mxCN3BsH7btm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Model Performance Monitoring**\n",
        "- For any deployed machine learning model, it's critical to keep track of how well it's performing. This is usually done by monitoring metrics like accuracy, prediction latency, or mean squared error (MSE). In our case study, we'll use MLflow to monitor model performance and log metrics.\n",
        "\n",
        "**b. Application Monitoring**\n",
        "- When deploying models through a Flask API, we need to monitor the API performance (e.g., how many requests are being made, response times, errors). For this, tools like Prometheus or Flask’s logging module can be used to track the status of requests and errors.\n",
        "\n",
        "**c. Infrastructure Monitoring**\n",
        "- For an automated pipeline using Apache Airflow, monitoring the workflow status (e.g., whether tasks succeeded, failed, or are running) is vital. Airflow comes with built-in logging and monitoring features through its web UI.\n",
        "\n"
      ],
      "metadata": {
        "id": "WLS_TtNXvK8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1. Model Monitoring with MLflow**\n"
      ],
      "metadata": {
        "id": "XYNoy3sIvWVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLflow Monitoring:**\n",
        "- MLflow is already integrated during the model training step to log metrics. Once the model is deployed, the API can also log each prediction to track how the model performs on real-world data over time.\n",
        "\n",
        "**MLflow Logging Example:**"
      ],
      "metadata": {
        "id": "bytID0Ze7g3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/deploy.py (extended with logging)\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "import mlflow\n",
        "\n",
        "app = Flask(__name__)\n",
        "model = joblib.load('/content/Housing.csv')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.json['data']\n",
        "    prediction = model.predict(np.array(data).reshape(1, -1))\n",
        "\n",
        "    # Log prediction to MLflow\n",
        "    with mlflow.start_run():\n",
        "        mlflow.log_param(\"input_data\", data)\n",
        "        mlflow.log_metric(\"prediction\", prediction[0])\n",
        "\n",
        "    return jsonify({'prediction': prediction.tolist()})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhxsPojAvjol",
        "outputId": "7e1348d1-cb26-45c7-e21e-a961e1c00ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How it works:**\n",
        "- **Logging Predictions:** Every time the API is hit, the input data and the prediction result are logged in MLflow.\n",
        "- **Tracking in MLflow UI:** You can open the MLflow UI by running mlflow ui and track predictions over time.\n",
        "\n",
        "**Metrics Monitored in MLflow:**\n",
        "- **Model Accuracy:** During training, accuracy (or other metrics like MSE) are logged.\n",
        "- **Prediction:** For each API call, we log the prediction result.\n",
        "- **Latency:** You could add code to measure how long the prediction takes and log it as a custom metric.\n",
        "Additional Metrics to Monitor:\n",
        "- **Data Drift:** Compare live data distributions to training data to detect shifts.\n",
        "- **Request Latency:** Log the time taken by the model to respond to each request.\n"
      ],
      "metadata": {
        "id": "u6fsP7GEvuie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2. Logging in Flask for API Monitoring**\n"
      ],
      "metadata": {
        "id": "Hq92Fn2swDvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Flask’s built-in logging features allow you to log important information like API requests, errors, and warnings.\n",
        "\n",
        "**Example for Flask API Logging:**"
      ],
      "metadata": {
        "id": "PQkmTbIl7lB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/deploy.py (with Flask logging)\n",
        "import logging\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, filename='logs/flask_api.log',\n",
        "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "model = joblib.load('/content/Housing.csv')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        data = request.json['data']\n",
        "        logging.info(\"Received data for prediction: %s\", data)\n",
        "\n",
        "        prediction = model.predict(np.array(data).reshape(1, -1))\n",
        "        logging.info(\"Prediction result: %s\", prediction)\n",
        "\n",
        "        return jsonify({'prediction': prediction.tolist()})\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error occurred: %s\", e)\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8fSeGchvj4g",
        "outputId": "e0fabfa3-3ddb-4821-ebd9-9ced61f77062"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How it works:**\n",
        "- **Request Logging:** Every incoming request is logged, including the input data.\n",
        "- **Prediction Logging:** After each prediction, the result is logged.\n",
        "- **Error Logging:** Any exceptions are caught and logged with error details.\n"
      ],
      "metadata": {
        "id": "HgSivKsjwWRz"
      }
    }
  ]
}